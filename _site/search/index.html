<!DOCTYPE html>

<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<link rel="stylesheet" href="/css/screen.css">
		<link rel="apple-touch-icon" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" href="/touch-icon.png" sizes="192x192">
		<link rel="icon" type="image/png" href="/images/favicon.png">
		<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Merriweather:400italic,400,300italic,300,700,700italic|Open+Sans:400italic,600italic,700italic,700,600,400|Inconsolata:400,700">

		

		<!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Search | OD Groundbreakers</title>
<meta name="generator" content="Jekyll v3.7.2" />
<meta property="og:title" content="Search" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We write tutorials about Cloud Native, DevOps, Blockchain, Microservices, Oracle Cloud Infrastructure and more." />
<meta property="og:description" content="We write tutorials about Cloud Native, DevOps, Blockchain, Microservices, Oracle Cloud Infrastructure and more." />
<link rel="canonical" href="http://localhost:4001/search/" />
<meta property="og:url" content="http://localhost:4001/search/" />
<meta property="og:site_name" content="OD Groundbreakers" />
<script type="application/ld+json">
{"description":"We write tutorials about Cloud Native, DevOps, Blockchain, Microservices, Oracle Cloud Infrastructure and more.","@type":"WebPage","url":"http://localhost:4001/search/","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4001/siteicon.png"}},"headline":"Search","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

		<link type="application/atom+xml" rel="alternate" href="http://localhost:4001/feed.xml" title="OD Groundbreakers" />

		<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
	</head>

	<body class="">
		<header>
			<div class="wrapper">
				<section class="top-bar">
					<div class="logo"><a href="/"><img src="/images/groundbreakerslogo.png" alt="Oracle Groundbreakers" height="180" width="240px">
</a></div>
					<a class="nav-toggle" id="open-nav" href="#">&#9776;</a>
<nav>
	<a class="editor-link btn" href="cloudcannon:collections/_data/navigation.yml" class="btn" style="padding: 5px;"><strong>&#9998;</strong> Edit navigation</a>
	
	

		
		<a href="/" class="">Tutorials</a>
	
	

		
		<a href="/codesamples/" class="">Code Samples</a>
	
	

		
		<a href="/designpatterns/" class="">Design Patterns</a>
	
	

		
		<a href="/events/" class="">Events</a>
	
</nav>

				</section>
				<section class="hero_search">
					<h1>Tutorials</h1>
					<p>Blogposts, Tutorials, Code Samples & Videos. All you need to get started.</p>
					<form action="/search/" method="get">
	<input type="search" name="q"  placeholder="What would you like to know?" autofocus>
	<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"/>
    <path d="M0 0h24v24H0z" fill="none"/>
</svg>
	<input type="submit" value="Search" style="display: none;">
</form>
				</section>
			</div>

		</header>
		<section class="content">
			<div class="wrapper">
				<p><span id="search-process">Loading</span> results <span id="search-query-container" style="display: none;">for "<strong id="search-query"></strong>"</span></p>
<ul id="search-results"></ul>

<script>
	window.data = {
		
			
				
					
					
					"weblogic-kubernetes-samples": {
						"id": "weblogic-kubernetes-samples",
						"title": "Weblogic Operator for K8s - Samples",
						"categories": "",
						"url": " /weblogic-kubernetes-samples/",
						"content": "These samples provide simple demonstrations of how to accomplish common tasks.\nThe samples are not intended to be used in production deployments or to be depended upon to\ncreate production environments.  They are provided for educational and demonstration purposes only.\n\nWhile these samples may be useful and usable as is, it is intended that you would read through all of the sample code in detail, understand how the given sample works, and then customize it to suit your needs.\n\nSample scripts\n\n\n  Sample for creating a Kubernetes secret that contains the Administration Server credentials. This secret can be used in creating a WebLogic domain resource.\n  Sample for creating a PV or PVC that can be used by a domain resource as the persistent storage for the WebLogic domain home or log files.\n  Sample for creating a WebLogic domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated WebLogic domain.\n  Sample for creating a WebLogic domain home inside a Docker image, and the domain resource YAML file for deploying the generated WebLogic domain.\n  Sample for configuring the Elasticsearch and Kibana deployments and services for the operator’s logs.\n  Sample for generating a self-signed certificate and private key that can be used for the operator’s external REST API.\n  Sample for creating an OKE cluster using Terraform.\n\n\nSample Helm charts\n\n\n  Sample Traefik Helm chart for setting up a Traefik load balancer for WebLogic clusters.\n  Sample Voyager Helm chart for setting up a Voyager load balancer for WebLogic clusters.\n  Sample Ingress Helm chart for setting up a Kubernetes Ingress for each WebLogic cluster using a Traefik or Voyager load balancer.\n  Sample Apache  Helm chart and Apache samples using the default or custom configurations for setting up a load balancer for WebLogic clusters using the Apache HTTP Server with WebLogic Server Plugins."
					}
					
				
			
		
			
				
					,
					
					"blockchain-everything-you-need-to-know-to-get-started-in-the-fascinating-world-of-blockchain": {
						"id": "blockchain-everything-you-need-to-know-to-get-started-in-the-fascinating-world-of-blockchain",
						"title": "Everything you need to know to get started in the fascinating world of Blockchain",
						"categories": "Blockchain",
						"url": " /blockchain/everything-you-need-to-know-to-get-started-in-the-fascinating-world-of-blockchain/",
						"content": "Absolute truth does not exist … or does it?\n\nYou are probably wondering if you have finally found an article in which the basic concepts of this technological trend are explained in a simple way. Usually, we find terms such as “Proof of Work”, “Smart Contract”, or “Hyperledger”. However, do we know what they really mean? And above all, what paradigm changes they will bring?\n\n\n  In our day to day we usually read quotes such as: “What the internet did for communications, blockchain will do for trusted transactions” — Forbes, or “The technology most likely to change the next decade of business is not the social web, big data, the cloud, robotics or even artificial intelligence. It is the Blockchain “- Harvard Business Review. But, will Blockchain really impact in our lives as Internet did?\n\n\nAfter some time researching around the ins and outs of Blockchain through different sources, I think I have concentrated the most important points to understand the big possibilities of this technology, which will transform sectors such as financial, energy or security. It will stir up any organization in which transactions are usually used, ultimately.\n\nFirst things first … What is Blockchain?\n\nBlockchain, also known as DLT (Distributed Ledger Technology) is a decentralized mechanism that keeps track of all transactions made between two or more nodes in a network, in a secure and distributed way. It is nothing more than a great accounting book that is replicated in the machines of all the participants of a network, making impossible to modify the information contained in it, and which only users with permission can access, turning it into a shared source of truth.\n\n\n\nThe mystery … Who created Blockchain?\n\nAll great stories hide mysteries, conspiracy theories, and different enigmas. Blockchain has all the necessary ingredients to turn its birth into a legend.\n\nOn Halloween 2008, several hundred people belonging to different cryptographic communities received an email with the following sender: Satoshi Nakamoto. He informed them that he had been working on a system that eliminated the need for trust intermediaries when making transactions, and solved the problem of double spending. All this was explained in the paper: “Bitcoin: A Peer-to-Peer Electronic Cash System”.\n\nThe first cryptographic currency based on Blockchain was created. And unlike most entrepreneurs and CxOs of the world’s largest multinationals, which feed the dreamers with their success stories in their search of success, its creator decided to remain anonymous.\n\nBut … What did Satoshi Nakamoto explain in his paper?\n\nUntil recently, whenever we needed to make a transaction, we had some central authority or third party that acted as an intermediary. Whether it was in the payment of an asset online, in the change of a property’s name, or when patenting a product, we relied on the truthfulness of banks, notaries or any other organization.\n\nBlockchain involves a paradigm shift in this system, by introducing a peer-to-peer exchange mechanism in a distributed and secure way, thanks to the cryptographic algorithm on which it is based.\n\nWhen several actors that are part of a Blockchain distributed network decide to exchange value (cryptographic currency, property title, etc.), a new block composed of a set of transactions is sent to all the nodes that participate in the network. At that moment, those nodes initiate the verification process, which consists in verifying that the information contained in the block is valid and that the bases of the agreed terms have not been altered. Once the majority of the nodes have validated it, the block is sealed with a cryptographic hash (algorithm) based on the information that contains and the previous block hash. Thus, the manipulation or alteration of the data contained in each of the blocks that are part of the chain is fairly impossible, and an immutable record is obtained, a source of truth.\n\nIn an effort to get a large number of nodes involved in the verification process, Satoshi Nakamoto proposed a system of miners. These users keep a copy of the chain in their machines, from where they use its compute capacity to calculate the complex mathematical operations necessary to incorporate a new block in the chain. Then, those miners who get the solution, obtain a great monetary reward. This process is called Proof of Work.\n\n\n\nMoreover, in most data centers around the world, the databases are replicated to prevent a single point of failure and to provide high availability to the service. It is an expensive solution, but essential for any organization that handles sensitive data. In the case of Blockchain and due to its distributed nature, all the nodes that are part of the network have a copy of the blocks, avoiding a single point of failure and making insignificant the probability of data loss.\n\n“Permissionless” and “Permissioned” Blockchain\n\nEven though one of the first applications of Blockchain was Bitcoin, many other implementations of this technology have already been found. In the case of cryptographic currencies, the Blockchain network is public. Any user can join and obtain a copy of all transactions in the chain and participate in the verification process. On the other hand, it is also possible to create a private Blockchain network, in which only users with an invitation have access to the information contained in it and can participate in the validation process.\n\nCurrently, several open source initiatives such as Ethereum, Corda or Hyperledger Fabric, have developed their own implementations of Blockchain, making life easier for developers by providing different tools and frameworks. Other private companies, such as Oracle, have also put in place solutions based on this powerful technology.\n\nLet’s give an example\n\nOnce we have an understanding of how a Blockchain public network such as Bitcoin works, let’s see one practical application of a private network. On this link, you will find the complete analysis of this case, which was modeled using the Blockchain service in the Oracle Cloud.\n\nTo improve the components’ traceability of their vehicles, a car manufacturer has integrated Blockchain with four of its dealers, creating the following network:\n\n\n\nThe car manufacturer is the founder of the Blockchain network, and the four dealers participate in it:\n\n\n\nWhenever a vehicle’s component is sold by one of the suppliers to the manufacturer, and once the transaction is approved by both parties, the transaction is registered in the Blockchain network. Thus, a large database shared between the parties is obtained, in a secure and unalterable way. Moreover, the dealers only have access to the information of their components, but not to those of other dealers.\n\n\n\nThis is just a simple practical example, that helps to understand better the countless applications that Blockchain has. For example, this solution in place would have avoided the scandal over the illegal installation of software that Volkswagen carried out to eleven million vehicles, in order to manipulate the results of the technical controls of polluting emissions. With a source of truth like Blockchain, it would have been possible to track the supply chain of all the cars, from the manufacturer to the customer, avoiding the manipulation in any of its phases.\n\nLast but not least … What are the Smart Contracts?\n\nConventionally, contracts are used to close agreements between two or more parties. In Blockchain’s world, the Smart Contracts define the rules of the agreed terms and execute the outcomes automatically, without the need for an intermediary institution. The Smart Contracts are scripts that can be programmed in different programming languages such as Java or Go, and that verify that when any of the conditions described in the contract is satisfied, the transaction is made.\n\nImagine that you decide to bet five euros on the fact that Real Madrid will win the next game against Barcelona, and another person bets the same amount to the opposite case. With a Smart Contract in place between the two parts, the result of the game would be verified automatically, and the money would be sent to the winner. In this sense, no intermediaries neither a third party would be needed to act as an arbitrator.\n\nAbsolute truth does not exist … or does it?\n\nBlockchain is the first source of distributed and inalterable truth. A large public database globally. Secure and collaborative.\n\nThere is no doubt that it is time to get on board with this amazing technology, in which the developer’s community will have a decisive role, not only in its growth but also in its transformation and evolution.\n\nThis sounds great, but … How can I get started and try it?\n\nShortly, the Blockchain service will be available in the Oracle Cloud within the free 30-days Trial.\n\nBut in the meanwhile, there are many other solutions available to test. Why not try them?\n\nSign Up for Free!\n\n\n\n\n\n\n  Raquel Teresa @Raquel_Teresa\n\n  Cloud Sales Consultant and Developer Advocate @ Oracle Digital.\nPassionate about technology and people."
					}
					
				
			
		
			
				
					,
					
					"miscellaneous-creating-blog-jekyll": {
						"id": "miscellaneous-creating-blog-jekyll",
						"title": "Creating a blog with Jekyll and GitHub Pages",
						"categories": "Miscellaneous",
						"url": " /miscellaneous/creating-blog-jekyll/",
						"content": "How many of you had the need to create a quick blog for a school project, a\nhackathon, or a work project but had no time to learn how to use a new complex\nContent Management System (CMS) like Drupal or\nWordpress from scratch?\n\nAs the new Developer Advocacy team @\nOracle\nwas created, we wanted to make sure we have a reliable way to publish our own\nstories, tutorials, code samples, and reference architectures.\n\nPublishing content on Medium or similar platforms is great, but we would also\nlike to start using our own “branding” — therefore we were looking for something\nlike a customizable CMS but easier to use and quicker to deploy.\n\nWe have found an easy solution to achieve our goal: Jekyll\n+ GitHub Pages.\n\nJekyll\n\nJekyll is a simple, extendable, static site generator. It allows you to write\ncontent with your favorite markup language, e.g. Markdown, and generate a\nnavigable website in seconds.\n\nThrough themes and templates (or layouts), it is possible to customize the\n“look&amp;feel” of your site, as well as the navigation and additional features like\nsocial media sharing. It also allows you to have a (client-side JavaScript\nbased) Search functionality which is something great you would not expect from a\nstatic HTML website.\n\nInstalling Jekyll on your machine it’s very easy and you can find the quick\nstart documentation here.\n\nWindows users\n\nWindows users are required to install some additional dependencies for Jekyll to\nwork properly. In order to do so, make sure to add the following lines to the\nGemfile:\n\ngem 'tzinfo' if Gem.win_platform?\ngem 'tzinfo-data' if Gem.win_platform?\ngem 'wdm', '&gt;= 0.1.0 if Gem.win_platform?\n\n\nThis will allow installing additional Ruby gems that are needed on Windows only.\nAfter adding these lines, make sure to run bundle update. Additional\ninformation can be found\nhere.\n\nGitHub Pages\n\nGitHub Pages is a static site hosting service designed to host your personal,\norganization, or project pages directly from a GitHub repository. It is possible\nto host a Jekyll generated website for free just using a regular GitHub\nrepository.\n\nAfter creating a new GitHub repository, it is possible to enable the GitHub Page\nfunctionality through its settings and your website will be immediately\navailable at *https://.github.io*. You only need to make sure the\nrepository name ends with “github.io”.\n\n\n\nIn our case, the website is available at\nhttps://oci-tutorials.github.io, while the\nsource content is stored at the corresponding repository\nhttps://github.com/oci-tutorials/oci-tutorials.github.io.\n\nIf you browse to the GitHub repository, you can see it contains a bunch of\nstatic HTML files, CSS, and YAML files. HTML is generated by Jekyll starting\nfrom the Markdown files added to the _posts folder.\n\n\n\nFinal result\n\nAs you can see below, this is the site generated from our GitHub repository\ncontaining Markdown content. Within a few hours, we were able to have a new site\nup and running, which will enable us to reach out as many developers as we can.\n\n\n\nIf you have any suggestion or feedback (or you have found any bugs), feel free\nto reach out and let us know.\n\n\n\n\n  Luca Iannario @liannario\n\n  Developer Advocate at Oracle. \nWhen I am not on the clouds, I like travelling and taking pictures."
					}
					
				
			
		
			
				
					,
					
					"serverless-future-of-serverless": {
						"id": "serverless-future-of-serverless",
						"title": "Future of Serverless",
						"categories": "Serverless",
						"url": " /serverless/future-of-serverless/",
						"content": "Future of Serverless\n\n\n\nYou’ve heard of Artificial Intelligence, IoT and Blockchain …\n\nWhat about “Serverless” computing? You will hear this term being thrown around a lot, as a buzzword, but to sum up, it just means that a cloud provider will be allowing developers to focus more on developing applications by offering a serverless platform. In an interview with InformationWeek.com, Joe Emison the CTO of BuildFax said :\n\n  “Software is eating the world. Companies that can develop software most effectively will be among those that succeed”\n\n\nI don’t think anybody can argue with this statement. So why has Serverless architecture become this hidden technology that only developers know about? What if I told you many are already seeing benefits from Serverless-based workflows, and in the near future, this type of architecture could power practically the whole internet?\n\nWhere did it start?\n\nBefore we dive straight into the big bad world of Serverless architecture, it will be easier to understand where it came from. Ever heard of a virtual machine? Well it’s simply a way to have more than one operating system on your host machine. It’s really useful, especially if you’re working on a few different projects which require different environments. The only bad thing? The amount of resources a virtual machine required became a problem.\n\nThe solution to this was Containerization. If you want to learn more about containers follow my previous article.\n\nWhat is serverless? The no-servers myth\n\nServerless is a cloud execution model. The name Serverless itself is a bit misleading — as of course there are servers managed by the companies or individuals cloud provider. You’ve all heard of Iaas, Paas, Saas…. What about Faas? Function as a Service — The idea is that developers only pay-per-execution, and they don’t need to worry about scaling their applications or machine resources. They can spend their time (wait for it) –actually developing! This in turn means less time to market which is another benefit of developing on a serverless platform.\n\nThat all sounds great — but there are still some growing pains involved in the implementation process. This is because companies that have massive monolith architectures find it hard to change to a function driven system. And I’m not saying that this is an easy-breezy process, but once they have this new Serverless architecture, they open themselves up to a much more productive workflow. I’m not a professional developer myself (more of a technology enthusiast). However, I firmly believe that any new technology takes time to grow out of the awkward stages — after all, doesn’t every new technology go through this?\n\nIt is our responsibility to take a positive outlook on emerging tech and spread that positivity, and by doing this the future of Serverless technologies will be a bright one.\n\n\n\nFuture of Serverless\n\nIn 2017, the serverless architecture market was valued $3.20 billion, and was expected to grow to $14.93 billion by 2023, according to a report conducted by MarketsandMarkets.\n\nThis massive growth spurt projection isn’t that surprising, as the popularity of cloud-native services is only rising. I’ve no doubt that the future will be driven by containerized apps and pay-by-use functions being fired off in a cloud providers space by way of automation. Imagine a world where you didn’t need to worry about the physical maintenance of your hardware that helps develop and run all your applications. This has been and always will be the draw to cloud computing.\n\nSo keep reading and learning— if you are interested in disruptive technology you’ll want to keep an eye on serverless computing.\n\n\n\n\n  Miriam Keenan @mirknn\n\n  Digital &amp; Developer Advocate Intern at Oracle."
					}
					
				
			
		
			
				
					,
					
					"contributors-about-me-miriam": {
						"id": "contributors-about-me-miriam",
						"title": "Miriam Keenan",
						"categories": "Contributors",
						"url": " /contributors/about-me-miriam/",
						"content": "About me\n\nHey, welcome to our community.\n\nMy name is Miriam and I’m a Digital Advocate who recently joined the Developer Advocacy team here at Oracle Digital. I’m also studying to complete my BSc. in Information Technology at Technological University Dublin. I’m pretty obsessed with the world of technology.\n\nThe great thing about being part of this new team is that we’re all passionate about the power of cloud computing. Through our varied levels of experience and knowledge, we intend to enable developers in our community to get the most out of their creations.\n\nFrom me, you’ll get a first-hand experience of getting started with Oracle cloud services. I’m currently interested in making stuff with Python, Java, and web development frameworks like Flask. The services I want to focus on are Container Management and Serverless Functions – but I won’t limit myself as there might be more on the horizon!\n\nWatch this space for tutorials, discussions, videos and more.\n\n\n\n\n  Miriam Keenan @mirknn\n\n  Digital &amp; Developer Advocate Intern at Oracle."
					}
					
				
			
		
			
				
					,
					
					"serverless-what-is-faas": {
						"id": "serverless-what-is-faas",
						"title": "What is this “FaaS”?",
						"categories": "Serverless",
						"url": " /serverless/what-is-faas/",
						"content": "What is this “FaaS” thing everyone talking about? I don’t want a “microservice”, I want a big service that makes me lots of money!\n\nWhat is Function-as-a-Service (FaaS)?\n\nFaaS is the concept of serverless computing with serverless architectures.\nSoftware developers can use this to deploy an individual function (Piece of\ncode), that performs an action, or piece of business logic, without worrying\nabout where they are running it (eg. serverless). They are expected to start\nwithin milliseconds and process individual requests and then the process ends.\nSounds simple right?\n\nPrinciples of FaaS:\n\n\n  Completely takes the pain of servers and environment constraints away from the\ndeveloper\n  Billing based on consumption and executions, not server instance sizes\n  Services that are event-driven and instantaneously scalable\n\n\n\n\nAt the basic level, you could describe them as a way to run some code when a\n“thing” happens. Shows how easy it is to process an HTTP request as a\n“Function”.\n\n\n\nBenefits &amp; Use Cases\n\nLike most things, FaaS is not going to be perfect for every app.\n\nIn general, we see companies and developers using them mostly for our very high\nvolume transactions so that they can scale when needed and don’t have to have\nredundant servers the rest of the time.\n\n\n  High volume transactions — Isolate them and scale them\n  Dynamic or burstable workloads — If you only run something once a day or month,\nno need to pay for a server 24/7/365\n  Scheduled tasks — They are a perfect way to run a certain piece of code on a\nschedule, think cron jobs.\n\n\n\n\nTypes of Functions\n\nThere are a lot of potential uses for functions. Below is a simple list of some\ncommon scenarios. Support and implementation for them vary by the provider or\nservice your using.\n\n\n  Scheduled tasks or jobs.\n  Process a web request.\n  Process queue messages.\n  Run manually.\n\n\n\n\nSummary\n\nDevelopers hate servers and server management. The idea of serverless\narchitectures is a dream came true for most developers. That said, I can’t see\nFaaS as being a complete replacement for normal application architectures. For\nexample a basic web application, it would take a lot of functions.\n\nIn my humble opinion, function-based apps are a perfect fit for replacing\nmicroservice style architectures and smaller high volume back-end services.\n\n\n  Serverless\n  Cloud Computing\n  Developer\n  Programming\n  Introduction\n\n\n\n  Brian Mathews @DevOps4Days"
					}
					
				
			
		
			
				
					,
					
					"serverless-serverless-fuctions-with-fn": {
						"id": "serverless-serverless-fuctions-with-fn",
						"title": "Serverless Functions on OCI using FN",
						"categories": "Serverless",
						"url": " /serverless/serverless-fuctions-with-fn/",
						"content": "Serverless Functions on Oracle Cloud using the open source FN project\n\n\nFN logo\n\nFn Project\n\nThe Fn project is an open source, container native, and\ncloud agnostic serverless platform which is being funded by Oracle. It’s easy to\nuse, supports every programming language, and in my opinion, performs very well.\n\nIntroduction\n\nAs this is an open source project as opposed to a lot of “as a service”\nofferings available, we are left with the first task of, where we would like to\nrun the project.\n\nThe good thing about this, however, is that the FN project is very versatile so\nyou can run it almost anywhere from your laptop, to on-premise, to your\nfavourite cloud platform.\n\nIn this blog, I will be walking you through getting started with FN hosted on\nOracle Clouds OKE (Kubernetes Engine). There are 2 reasons which I chose this as\na platform for testing firstly the easy access and use of the OKE service for a\nfully managed Kubernetes service and secondly its completely free to test with a\nfree trial!\n\nGetting started\n\nFor getting started with FN on Kubernetes the team at FN have made it really\neasy by releasing a Helm chart to give easy installation. This chart deploys a\nfully functioning instance of the Fn platform\non a Kubernetes cluster using the Helm package manager.\n\nPrerequisites\n\n\n  PV provisioner support in the underlying infrastructure (for persistent data,\nsee below )\n  Install Helm\n  Initialise Helm by installing Tiller, the server portion of Helm, to your\nKubernetes cluster\n\n\nLet get started:\n\nDownload helm using a package manager for ease:\n\nInitialise Helm once installed:\n\nhelm init\n\n\nInstalling the Chart\n\nClone the fn-helm repo:\n\ngit clone \n &amp;&amp; cd fn-helm\n\n\nInstall chart dependencies from\nrequirements.yaml:\n\nhelm dep build fn\n\n\nThe default chart will install fn as a private service inside your cluster with\nephemeral storage, to configure a public endpoint and persistent storage you\nshould look at\nvalues.yaml\nand modify the default settings. For OKE on Oracle Cloud it is as follows:\n\nfn:                         \nservice:                            \nannotations:                               service.beta.kubernetes.io/oci-load-balancer-shape: 400Mbps                       ui:                         \nservice:                            \nannotations:                                 service.beta.kubernetes.io/oci-load-balancer-shape: 400Mbps                                               mysql:                          \npersistence:                             \nenabled: true                             \nexistingClaim: tc-fn-mysql                                               redis:                          \npersistence:                             \nenabled: true                             \nexistingClaim: tc-fn-redis                                                                       rbac:                         \nenabled: true\n\n\nOnce you have this edited for your relevant Kubernetes host we can go ahead and\ninstall the chart on Kubernetes.\n\nTo install the chart with the release name release:\n\nhelm install --name release fn\n\n\nNote:* if you do not pass the — name flag, a release name will be\nauto-generated. You can view releases by running helm list (or helm ls, for\nshort).*\n\nNow that FN has been installed we can start the service.\n\nfn start\n\n\nThe above command runs Fn in a single server mode with an embedded database and\nqueue. Behind the scenes, the fn start command runs a Docker image called\nfnproject/fnserver in a privileged mode. It also mounts the Docker socket into\nthe container as well as the /data folder in the current working directory\n(this is where database and queue information is stored). Finally, it exposes\nport 8080 to the host so you can invoke it on that port.\n\nFirst function\n\nThe FN CLI which we have installed has a built-in init command which we use\nfor new functions.\n\nFor FN &amp; Serverless functions there are a few terms which are useful to know\nthat I will refer too in the next steps.\n\nApps  Apps are a way to group your functions and triggers logically\nunder the same name e.g. test-app\n\nTriggers Triggers are pointers to functions used for invoking the\nfunction code. Think of it as an endpoint where the function can be invoked\nfrom, e.g.http://localhost:8080/test-app/helloworld which uses a HTTP trigger.\n**NOTE: **You can have multiple triggers pointing to the same function, eg\nevents, HTTP triggers, scheduled, etc.\n\nFunctionsThis is the piece of code you are writing and that gets executed.\n\nImages Docker image that packages your function; the image used depends\non the language of the function (e.g.fnproject/go, fnproject/ruby), for the\nbest performance of functions we want our images to be as small as possible to\nmake the function easier to scale.\n\nLet’s get started\n\nNow that we have the terminology sorted let’s get started and make our sample\napp.\n\nTo do this we use the init command to create a function with a Golang runtime\n--runtime go then the provide it with the trigger type --trigger http and\nthen finally the subfolder for the function in this case hello\n\nfn init --runtime go --trigger http hello\n\n\nWhen we go to the newly created function hello subfolder, in this folder we\ncan see the structure of the function, eg:\n\nhello\n├── Gopkg.toml\n├── func.go\n├── func.yaml\n└── test.json\n\n\nThe source code for your function is in the func.go file and has a function\nhandler that responds with a “Hello World” message. The func.yaml file\ncontains information such as version runtime, name, entry point for your\nfunction and a list of triggers.\n\nYou may look at the test.json file and wonder what it is, this file holds an\narray of tests (input values and expected output values) that can be used to\ntest your function as a black-box service fn test. This is really useful\nfortesting any services in the beta stage.\n\nNow that we have created our function using the built-in hello example we can\nrun the function for testing using the fn run command.\n\n**Note: **Before you run the function make sure you set the FN_REGISTRY\nenvironment variable to your Docker repository.\n\nNow when you run the command, Fn will build the Docker image with the function\nand runs the function like this:\n\nfn run\n\nBuilding image hello:0.0.1 ...........\n{\"message\":\"Hello World\"}\n\n\nThis is all great, but we have the Fn server running locally, so let’s deploy\nour function to the server, instead of just running it.\n\nTo deploy the function, you can use the fn deploy command and specify the app\nname. Note that you need to run the command below from within the function\nfolder:\n\nfn deploy --app myapp\n\n\nThis command deploys the app (called myapp) and a function called hello to\nthe local Fn server and links a trigger called hello-trigger to that function.\n\nThis means that on the Fn server, the function will be accessible under\n/myapp/hello-trigger path (e.g. http://**ClsuterIP**/t/myapp/hello-trigger).\nThe app name is used to logically group functions together. To see the full list\nof triggers defined on the Fn server, just run this command:\n\n# List all triggers for 'myapp'\nfn list triggers myapp\n\nFUNCTION        NAME            TYPE    SOURCE          ENDPOINT\nhello           hello-trigger   http    /hello-trigger  \n\n\nFinally, if you access the endpoint, you will get back the “Hello World”\nmessage like this:\n\ncurl \n{\"message\":\"Hello World\"}\n\n\nGrouping functions\n\nTo group the functions together, we can use the app name construct — this\nallows you to group different functions together logically (e.g. test-app\ncould have functions such as hello and goodbye).\n\nIn this case, the test-app could also be the folder where your functions live,\nwith their subfolders eg. /hello and /goodbye which contain the actual\nfunctions. You can also define the app.yaml file in the app root folder, to be\nable to deploy all functions with one command.\n\nFollow the steps below to create a test-app with hello and goodbye functions:\n\n# Create the test-app folder\nmkdir test-app\ncd test-app\n\n# Create app.yaml that defines the app name\necho \"name: test-app\" &gt; app.yaml\n\n# Create a hello function in /hello subfolder\nfn init --runtime go --trigger http hello\n\n# Create a goodbye function in /goodbye subfolder\nfn init --runtime go --trigger http goodbye\n\n\nNext, we can go into the /hello and the /goodbye folder, and deploy each app\nseparately with fn deploy --app test-app. Alternatively, since we have the app\nname defined in app.yaml in the root folder, we can use this command to deploy\nall functions to the Fn server:\n\nfn deploy --all\n\n\nThe above command creates the following functions and triggers:\n\nfn list triggers test-app\n\nFUNCTION        NAME            TYPE    SOURCE                  ENDPOINT\ngoodbye         goodbye-trigger http    /goodbye-trigger        \nhello           hello-trigger   http    /hello-trigger          \n\n\nYou can also create a function that lives in the root of your app by running fn\ninit command from the apps root folder:\n\nfn init --runtime node --trigger http\n\n\nThen deploy it again:\n\nfn deploy --all\n\n\nNow we have three triggers defined under the greeter-app logical group:\n\nfn list triggers test-app\n\nFUNCTION        NAME                    TYPE    SOURCE                  ENDPOINT\ngoodbye         goodbye-trigger         http    /goodbye-trigger        \ngreeter-app     test-app-trigger     http    /greeter-app-trigger    \nhello           hello-trigger           http    /hello-trigger          \n\n\nEnabling the UI\n\nIf you prefer to interact with a UI, you can also do this with FN. Assuming you\nhave the Fn server running locally, you can start the UI l:\n\ndocker run --rm -it --link fnserver:api -p 4000:4000 -e \"FN_API_URL=\n\" fnproject/ui\n\n\nWhen the image has been downloaded and the container has been executed, you’ll\nbe able to access the UI on http://localhost:4000.\n\nTry it yourself for free!\n\nNow that you’ve seen how FN works and how to get started and host it on the\ncloud, why not give it a go yourself with up to 3500 hours worth of free credits\non Oracle Cloud http://bit.ly/brianMediumBlog\n\n\n  Brian Mathews @DevOps4Days"
					}
					
				
			
		
			
				
					,
					
					"packagejson": {
						"id": "packagejson",
						"title": "package.json",
						"categories": "",
						"url": " /packagejson/",
						"content": "{\n  \"name\": \"imagedims\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Function using ImageMagick that returns dimensions\",\n  \"main\": \"func.js\",\n  \"author\": \"fnproject.io\",\n  \"license\": \"Apache-2.0\",\n  \"dependencies\": {\n  \"@fnproject/fdk\": \"&gt;=0.0.11\",\n  \"tmp\": \"^0.0.33\",\n  \"imagemagick\": \"^0.1.3\"\n }\n}"
					}
					
				
			
		
			
				
					,
					
					"serverless-getting-started-with-customer-dockerfiles": {
						"id": "serverless-getting-started-with-customer-dockerfiles",
						"title": "Creating custom Dockerfiles for Node.js function’s",
						"categories": "Serverless",
						"url": " /serverless/getting-started-with-customer-dockerfiles/",
						"content": "Getting started with Custom Dockerfiles for Node.js for Serverless Function’s\n\n\n\nIntro:\n\nThe aim of this tutorial is to walk through how you can use a custom Docker\nimage to define an Node.js serverless function.\n\nFor the purpose of this tutorial I used the opensource Fn project for testing as\nyou can run it on your local machine. For more information on Fn see tee the\nfollowing page http://fnproject.io/ or take a look at my\nlast blog which walks you through configuring Fn to run on Kubernetes on a free\nOCI Cloud\ntrial.\n\nFor the demo function I thought I would try something and use\nImageMagick to do some nice simple image\nprocessing in our function and while there is a Node.js module for ImageMagick,\nit’s really just a wrapper on the underlying native library. So we’ll have to\ninstall the library in addition to adding the Node module to our package.json\ndependencies. Let’s start by creating the Node function\n\nPrequisites\n\nThis tutorial requires you to have both Docker and Fn installed. If you need\nhelp with Fn installation you can find instructions in the Install and Start Fn\nTutorial.\n\nGetting Started\n\nIf it isn’t already running, you’ll need to start the Fn server. We’ll run it in\nthe foreground to let us see the server log messages so let’s open a new\nterminal for this.\n\nStart the Fn server using the fn cli:\n\nFn start\n\n\nFunction Definition\n\nFirstly we shall create a folder for our deployment package:\n\nmkdir magick-function\n\n\nWe shall now jump into that folder:\n\ncd magick-function\n\n\nIn an empty folder create a file named func.js\n\nnano func.js\n\n\nIn this function you can paste the following as its content:\n\nconst fdk = require('@fnproject/fdk');\nconst fs  = require('fs');\nconst tmp = require('tmp');\nconst im  = require('imagemagick');\n\nfdk.handle((buffer, ctx) =&gt; {\n  return new Promise((resolve, reject) =&gt; {\n    tmp.tmpName((err, tmpFile) =&gt; {\n      if (err) throw err;\n      fs.writeFile(tmpFile, buffer, (err) =&gt; {\n        if (err) throw err;\n        im.identify(['-format', '{\"width\": %w, \"height\": %h}', tmpFile],\n          (err, output) =&gt; {\n            if (err) {\n              reject(err);\n            } else {\n              resolve(JSON.parse(output));\n            }\n          }\n        );\n      });\n    });\n  });\n}, { inputMode: 'buffer' });\n\n\nThis function is pretty straight forward, it takes a binary image as it’s\nargument, writes it to a tmp file, and then uses ImageMagick to obtain the width\nand height of the image. Since the function argument type is binary we need to\nset the “inputMode” property to “buffer” when we call the the FDK’s handle\nfunction.\n\nDeclaring Node.js Dependencies\n\nThere are some interesting elements to this function, but the key one for us is\nthe use of the “imagemagick” Node module for image processing.\n\nTo use it we need to include it in our dependencies in the package.json along\nwith the other dependencies.\n\nIn same folder as the func.js file, create a package.json file and paste the\nfollowing as its content:\n\n{\n\n\"name\": \"imagedims\",\n\n\"version\": \"1.0.0\",\n\n\"description\": \"Function using ImageMagick that returns dimensions\",\n\n\"main\": \"func.js\",\n\n\"author\": \"fnproject.io\",\n\n\"license\": \"Apache-2.0\",\n\n\"dependencies\": {\n\n\"@fnproject/fdk\": \"&gt;=0.0.11\",\n\n\"tmp\": \"^0.0.33\",\n\n\"imagemagick\": \"^0.1.3\"\n\n}\n\n}\n\n\nLike all Node.js functions using the Fn Node FDK we include it as a dependency\nalong with the “tmp” module which as I mentioned we need for the temporary file\nutilities and “imagemagick” for image processing.\n\nFunction Metadata\n\nNow that we have a Node.js function and it’s dependencies captured in the\npackage.json we need a func.yaml to capture the function metadata.\n\nIn the folder containing the previously created files, create a func.yaml file\nand paste the following as its content:\n\nschema_version:\n20180708\n\nname:\nimagedims\n\nversion:\n0.0.1\n\nruntime:\ndocker\n\ntriggers:\n\n-\nname:\nimagedims-trigger\n\ntype:\nhttp\n\nsource:\n/imagedims\n\n\nNote: **This is a pretty typical func.yaml for a Node.js function,\n**except that instead of declaring the runtime as “node” we’ve specified\n“docker”. If you were to type fn build right now you’d get the error:\n\n$ Fn: Dockerfile does not exist for ‘docker’ runtime\n\n\nThis is because when you set the runtime type to “docker” fn build defers to\nyour Dockerfile to build the function container image–and we haven’t defined\nthis yet.\n\nDefault Node.js Function Dockerfile\n\nThe Dockerfile that fn build would normally generate to build a Node.js\nfunction container image looks like this:\n\nFROM fnproject/node:dev as build-stage\nWORKDIR /function\nADD package.json /function/\nRUN npm install\nFROM fnproject/node\nWORKDIR /function\nADD . /function/\nCOPY --from=build-stage /function/node_modules/ /function/node_modules/\nENTRYPOINT [\"node\", \"func.js\"]\n\n\nIt’s a two stage build with the fnproject/node:dev image containing npm and\nother build tools, and the fnproject/node image containing just the Node\nruntime. This approach is designed to ensure that deployable function container\nimages are as small as possible–which is beneficial for a number of reasons.\n\nCustom Node.js Function Dockerfile\n\nThe fnproject/node container image is built on Alpine so we’ll need to install\nthe ImageMagick Alpine package using the apk package management utility. You\ncan do this with a Dockerfile RUN command:\n\nRUN apk add --no-cache imagemagick\n\n\nWe want to install ImageMagick into the runtime image, not the build image, so\nwe need to add the RUN command after the FROM fnproject/node command.\n\nIn the folder containing the previously created files, create a file named\nDockerfile and paste the following as its content:\n\nFROM fnproject/node:dev as build-stage\nWORKDIR /function\nADD package.json /function/fn\nRUN npm install\nFROM fnproject/node\nRUN apk add --no-cache imagemagick\nWORKDIR /function\nADD . /function/\nCOPY --from=build-stage /function/node_modules/ /function/node_modules/\nENTRYPOINT [\"node\", \"func.js\"]\n\n\nWith this Dockerfile, the Node.js function, it’s dependencies (including the\n“imagemagick” wrapper), and the “imagemagick” Alpine package will be included in\nan image derived from the base fnproject/node image. We should be good to go!\n\nNow we can build our file:\n\nBuilding and Deploying\n\nOnce you have your custom Dockerfile you can simply use fn build to build your\nfunction. Give it a try:\n\nfn -v build\n\nYou should see output similar to:\n\nJust like with a default build, the output is a container image. From this point\nforward everything is just as it would be for any function. Since I previously\nstarted an Fn server, you can deploy it now and test. Let’s deploy to an\napplication named ‘tutorial’:\n\nfn deploy --app tutorial --local\n\nWe can confirm the function is correctly defined by getting a list of the\nfunctions in the “tutorial” application:\n\nfn list functions tutorial\n\nTip: The fn cli let’s you abbreviate most of the keywords so you can also\nsay fn ls f tutorial\n\nYou should see output similar to:\n\nNAME        IMAGE             ID\nimagedims   imagedims:0.0.1   01CWFAS9DBNG8G00RZJ0000002\n\n\nInvoking the Function\n\nWith the function deployed let’s invoke it to make sure it’s working as\nexpected. You’ll need a jpeg or png file so either find one on your machine or\ndownload one. I used a random photo on my laptop\n\ncat Test-image.jpg | fn invoke tutorial imagedims\n\nFor this file you should see the following output:\n\n{\"width\":720,\"height\":540}\n\n\n\nCalling the Function with curl\n\nYou may have noticed when creating the func.yamlwe included a HTTP trigger\ndeclaration, this is so we can also call the function easily with curl.\n\nIt’s a little more complicated as you need to declare the content type because\nthe request body is binary. You also need to use the --data-binary switch:\n\ncurl --data-binary Test-image.jpg -H \"Content-Type: application/octet-stream\" -X POST http://localhost:8080/t/tutorial/imagedims\n\nYou should get exactly the same output as when using fn invoke.\n\nConclusion\n\nOne of the most powerful features of Fn is the ability to use custom defined\nDocker container images as functions. This feature makes it possible to\ncustomize your function’s runtime environment including letting you install any\nLinux libraries or utilities that your function might need. And thanks to the Fn\nCLI’s support for Dockerfiles it’s the same user experience as when developing\nany function.\n\nHaving completed this tutorial you’ve successfully built a function using a\ncustom Dockerfile. Well done, why not give it a go now with Fn hosted on the\nKubernetes on OCI for free as outlined in my previous\nblog!\n\n\n  Brian Mathews @DevOps4Days"
					}
					
				
			
		
			
				
					,
					
					"funcjs": {
						"id": "funcjs",
						"title": "func.js",
						"categories": "",
						"url": " /funcjs/",
						"content": "const fdk = require('@fnproject/fdk');\nconst fs  = require('fs');\nconst tmp = require('tmp');\nconst im  = require('imagemagick');\n\nfdk.handle((buffer, ctx) =&gt; {\n  return new Promise((resolve, reject) =&gt; {\n    tmp.tmpName((err, tmpFile) =&gt; {\n      if (err) throw err;\n      fs.writeFile(tmpFile, buffer, (err) =&gt; {\n        if (err) throw err;\n        im.identify(['-format', '{\"width\": %w, \"height\": %h}', tmpFile],\n          (err, output) =&gt; {\n            if (err) {\n              reject(err);\n            } else {\n              resolve(JSON.parse(output));\n            }\n          }\n        );\n      });\n    });\n  });\n}, { inputMode: 'buffer' });"
					}
					
				
			
		
			
				
					,
					
					"func-yaml": {
						"id": "func-yaml",
						"title": "func.yaml",
						"categories": "",
						"url": " /func-yaml/",
						"content": "schema_version: 20180708\nname: imagedims\nversion: 0.0.1\nruntime: docker\ntriggers:\n- name: imagedims-trigger\ntype: http\nsource: /imagedims"
					}
					
				
			
		
			
				
					,
					
					"kubernetes-getting-started-with-istio": {
						"id": "kubernetes-getting-started-with-istio",
						"title": "Istio — Getting started guide",
						"categories": "Kubernetes",
						"url": " /kubernetes/Getting-started-with-istio/",
						"content": "Istio — Getting started with Configuring, Monitoring &amp; Managing your\nMicroservice Deployments on Kubernetes\n\n\nIstio\n\nWhat is Istio?\n\nIstio — https://istio.io — is a new Microservice service\nmesh manager for making microservice deployments less complex and eases the\nstrain on development teams. It is a completely open source service mesh that\nlayers transparently onto existing distributed applications. It is also a\nplatform which included APIs that let it integrate into any logging platform,\ntelemetry or policy system.\n\nWhat is a Service Mesh?\n\nA service mesh is a term we use to describe the network of microservices that\nmake up a distributed microservice architecture that makeup applications and the\ninteractions between them.\n\nAs these service meshes grow with size and complexity they become harder to\nunderstand and manage. It can require discovery, load balancing, failure\nrecovery, metrics, and monitoring. They can also have more complex operational\nrequirements such as A/B testing, canary releases, rate limiting, access\ncontrol, and end-to-end authentication.\n\n\nBooking info application service mesh\nhttps://istio.io/docs/tasks/telemetry/servicegraph/\n\nWhy use Istio?\n\nHow can we manage this service mesh complexity? In comes Istio which provides\nbehavioral insights and operational control over the service mesh as a whole,\noffering a complete solution to satisfy the requirements of microservice\napplications.\n\nIstio makes it easy to create a network of deployed services with load\nbalancing, service-to-service authentication, monitoring, and more, without any\nchanges in service code. To do this you add Istio support to services by\ndeploying a sidecar proxy throughout your environment that intercepts all\nnetwork communication between microservices. You then configure and manage Istio\nusing its control plane functions, such as:\n\n\n  · Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic.\n\n\n\n  · Fine-grained control of traffic behavior with rich routing rules, retries,\nfailovers, and fault injection.\n\n\n\n  · A pluggable policy layer and configuration API supporting access controls,\nrate limits and quotas.\n\n\n\n  · Automatic metrics, logs, and traces for all traffic within a cluster,\nincluding cluster ingress and egress.\n\n\n\n  · Secure service-to-service communication in a cluster with strong\nidentity-based authentication and authorization.\n\n\nTime to try it:\n\nIn this article, I will show the steps for how I got started with Istio on my\nKubernetes cluster that I provisioned on Oracle Cloud.\n\nSteps taken:\n\n\n  Install Istio client on a my Mac.\n  Deploy Istio to my Oracle Cloud OKE Kubernetes Cluster\n  Deploy the Book info sample application with Sidecar Injection\n  Try out some Istio functionalities — like traffic management and monitoring\n\n\nThe conclusion is that leveraging Istio on OKE is quite straightforward.\n\n\n  (the Envoy Sidecar is the proxy that is added to every Pod to handle all\ntraffic into and out of the Pod; this is the magic that makes Istio work)\n\n\nInstall Istio Client in Linux VM\n\nThe first step with Istio, prior to deploying Istio to the Kubernetes cluster,\nis the installation on your client machine of the istioctl client application\nand associated sources, including the Kubernetes yaml files required for the\nactual deployment.\n\nFollowing the instructions in the quick start guide:\nhttps://istio.io/docs/setup/kubernetes/quick-start.html\n\nFirstly I downloaded the latest version of Istio to my Mac\n\ncurl -L | sh -\n\n\n\nIstio Service Dashboard\n\nIstio release download\n\nThen move to the Istio directory:\n\ncd istio-1.0.2\n\n\nThe installation directory contains:\n\n\n  Installation .yaml files for Kubernetes in /install/\n  Sample applications in /samples/\n  The istioctl client binary in the bin/ directory. istioctl is used when manually\ninjecting Envoy as a sidecar proxy and for creating routing rules and policies.\n  The istio.VERSION configuration file\n\n\nYou then add the istioctl client to my PATH environment variables:\n\nexport=$PWD $PATH\n\n\nEnsure that the current OCI and OKE user is allowed to do cluster administration\ntasks by creating a Role-Based Access Control policy\n\n kubectl create clusterrolebinding &lt;admin-binding&gt; –clusterrole=cluster-admin –user=&lt;user-OCID&gt;\n\n\nNote:\n\n\n  admin-binding: Is any string that you want, such as “adminrolebinding”\n  user: Your user OCID\n\n\nDeploy Istio to your Oracle Cloud OKE Kubernetes Cluster\n\nI will cover the installation of Istio using Helm below, however, prior to\nperforming the installation, let’s make some changes to the Istio “values.yaml”\nfile. The “values.yaml” file informs Helm which components to install on the OKE\nplatform. The “values.yaml” file is located at:\n\nIn order to have the components Grafana, Prometheus, Servicegraph, and Jaeger\ndeployed, the “values.yaml” file needs to be modified. For each of the\ncomponents you want to be deployed, change the enabled property from “false” to\n“true” eg:\n\nServicegraph:\n\nenabled: true\n\nreplicaCount: 1\n\nimage: servicegraph\n\nservice:\n\nname: http\n\ntype: ClusterIP\n\nexternalPort: 8088\n\ninternalPort: 8088\n\n\nOnce you have made these changes to the “values.yaml” file you can save. and\nexit and you are now ready to install Istio!\n\nIf you are using a version of Helm prior to 2.10.0 then you must first install\nIstio’s Custom Resource Definitions via the kubectl apply:\n\nkubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml\n\n\nAfter this command execution you will have to wait a few seconds for the Custom\nResource Definitions (CRDs) to be committed in the kube-apiserver.\n\nOnce the CRDs have been added to the kube-apiserver we can now render Istio’s\ncore components to a Kubernetes manifest called istio.yaml:\n\nhelm template install/kubernetes/helm/istio --name istio --namespace istio-system &gt; $HOME/istio.yaml\n\n\nNow that the components have been rendered manifest file we can now install the\ncomponents via the manifest by:\n\nCreating the namespace in Kubernetes with Kubectl:\n\nkubectl create namespace istio-system\n\n\nWe then apply the istio.yaml file to install istio:\n\nkubectl apply -f $HOME/istio.yaml\n\n\nWe can now see that Istio has been installed to our pods by running:\n\nkubectl get pods -n istio-system\n\n\nSince the “values.yaml” was modified to enable the deployment of Grafana,\nPrometheus, ServiceGraph, and Jeager you will see those components deployed as\nwell.\n\nWhile Istio states there is automatic sidecar injection; there are some slight\nlimitations to this . Automatic sidecar injection must be specified per\nnamespace; therefore, if you do not enable your namespace for automatic\ninjection then the sidecar will not be injected into your pods. I do not\nrecommend enabling the default namespace for automatic sidecar injection.\n\nPlease note, when enabling automatic sidecar injections that there may in future\nbe some components that you deploy to the default namespace and you don’t want\nthe sidecar deployed alongside the component. It would be better to deploy your\napplication to a specified namespace and then set this namespace for automatic\ninjection.\n\nNote:\n\nIn order to have sidecar injection at deployment, you must enable the namespace\nfor your application. To enable the namespace for automatic injection execute\nthe following command:\n\nkubectl label namespace default istio-injection=enabled\n\n\nRunning the Book Information Application\n\nFor this demo, we are going to deploy the Istio provided “bookinfo” sample\napplication. You can find this application in the samples directory from the\nIstio download, which we downloaded earlier. Keep in mind that we previously\nenabled automatic sidecar injection during the installation of Istio and also\nenabled the default namespace for automatic sidecar injection. Therefore, when\nyou deploy the book application an Envoy sidecar proxy is deployed in each pod.\n\nEach of the black boxes in the below diagram are instances of the Envoy proxy\nsidecar. When the “bookinfo” application is deployed to the Kubernetes cluster\nIstio deploys the sidecar in the pod alongside of the microservice.\n\n\nIstio bookinfo architecture\nhttps://istio.io/docs/examples/bookinfo/\n\nNote:\n\n\n  This application is polyglot, i.e., the microservices are written in different\nlanguages. It’s worth noting that these services have no dependencies on Istio,\nbut make an interesting service mesh example, particularly because of the\nmultitude of services, languages and versions for the reviews service.\n\n\nLet’s deploy the bookinfo application.\n\nkubectl apply -f /&lt;istio installation directory&gt;/samples/bookinfo/platform/kube/bookinfo.yaml\n\n\nAfter the successful deployment, let’s take a look at the pods that were\ndeployed.\n\nWe can check our pods by running:\n\nkubectl get pods\n\n\nWe then need to make the application accessible from outside your Kubernetes\ncluster. To do that, we need to create an Istio gateway:\n\n&lt;istio installation directory&gt;/samples/bookinfo/networking\n\n\nWe can check this gateway by:\n\nkubectl get gateway\n\n\nWe can also check the cluster-ip and ports for this gateway by running:\n\nkubectl get svc -n istio-system\n\n\n\nGateway information output from “kubectl get svc -n istio-system”\n\nIstio is now installed on our cluster with the bookinginfo application, we can\nnow check out some of the monitoring functionality and dashboards available.\n\nAvailable Dashboards\n\nWhen you install Istio, with all of the dashboards enabled, there will be 4\ndashboards available, in addition to the standard Kubernetes dashboard. Each\ndashboard provides their own unique features and will be key for managing and\nmonitoring your Kubernetes cluster. Since each dashboard is a product in its own\nright I will not cover each product in depth. To understand the key features of\nthe dashboards I recommend that you review each product’s documentation page.\nThere are also several books that have been written on many of these products.\n\nGrafana\n\nThe Grafana add-on is a preconfigured instance of\nGrafana. The base image has been modified to\nstart with both a Prometheus data source and the Istio Dashboard installed. The\nbase install files for Istio, and Mixer in particular, ship with a default\nconfiguration of global metrics. The Istio Dashboard is built to be used in\nconjunction with the default Istio metrics configuration and a Prometheus\nbackend.\n\nThe Istio Dashboard consists of three main sections:\n\n\n  Individual Workloads View: This section provides metrics about requests and\nresponses for each individual workload within the mesh (HTTP/gRPC and TCP).\nAlso, give metrics about inbound workloads and outbound services for this\nworkload.\n  Individual Services View: This section provides metrics about requests and\nresponses for each individual service within the mesh (HTTP/gRPC and TCP). Also,\ngive metrics about client and service workloads for this service.\n  A Mesh Summary View: This section provides a Global Summary view of the Mesh and\nshows HTTP/gRPC and TCP workloads in the Service Mesh.\n\n\nSetup the dashboard:\n\nTo set up the Istio dashboard we will need to set up a port forward to the\ndashboard fro kubectl locally on your laptop:\n\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath=’{.items[0].metadata.name}’) 3000:3000 &amp;\n\n\nOnce this has been configured we can access the dashboard at the :\nhttp://localhost:3000/dashboard/db/istio-mesh-dashboard\n\nThe Istio service mesh delivers six default Grafana dashboards. Please see some\nscreenshots of these dashboards below:\n\n\n\nMesh Dashboard\n\n\n\nIstio Service Dashboard\n\n\n\nIstio Workload Dashboard\n\n\n\nIstio Mixer Dashboard\n\nThe Grafana add-on is a pre-configured instance of Grafana. The base image\n(grafana/grafana:4.1.2) has been\nmodified to start with both a Prometheus data source and the Istio Dashboard\ninstalled. The Istio Dashboard is built to be used in conjunction with the\ndefault Istio metrics configuration and a Prometheus backend. More details on\nPrometheus: https://prometheus.io/ .\n\nWhy not give this a try for yourself with a free Kubernetes instance hosted on\nOracle Cloud with $300 free credit\nhttp://bit.ly/brianMediumBlog\n\n\n  Kubernetes\n  Istio\n  DevOps\n  Cloud Computing\n  Deployment\n\n\n\n  Brian Mathews @DevOps4Days"
					}
					
				
			
		
			
				
					,
					
					"dockerfile-magick-function": {
						"id": "dockerfile-magick-function",
						"title": "Dockerfile - Magick function",
						"categories": "",
						"url": " /Dockerfile-magick-function/",
						"content": "FROM fnproject/node:dev as build-stage\nWORKDIR /function\nADD package.json /function/fn\nRUN npm install\nFROM fnproject/node\nRUN apk add --no-cache imagemagick\nWORKDIR /function\nADD . /function/\nCOPY --from=build-stage /function/node_modules/ /function/node_modules/\nENTRYPOINT [\"node\", \"func.js\"]"
					}
					
				
			
		
			
				
					,
					
					"contributors-about-me-brian": {
						"id": "contributors-about-me-brian",
						"title": "Brian Mathews",
						"categories": "Contributors",
						"url": " /contributors/about-me-brian/",
						"content": "About me\n\nHey Everyone, my name is Brian Mathews and I am part of Developer Advocacy team within Oracle, based in Dublin.\n\nMy background is in Software Engineering (BSc), I graduated a few years and went straight into a role in development and support in the same company where I had completed my Internship as part of my degree. I have experience with enterprise scale developments all the way down to smaller personal automation scripts for random things in my house\n\nHaving been born in Ireland and spending my life so far here I have always been used to clouds so naturally, I was drawn cloud computing.\n\nI have a passion for day to day automation of mundane household tasks, which really isn’t helped by having IoT integration with my Alexa readily available.\n\n\n\nI have recently started working in Oracle as part of the new Developer Advocacy team on the OCI platform.\n\nThe team spends our time trialing and creating some cool projects to see exactly what the OCI platform is capable of and showing the world the sort of cool application it can have for them. This ranges for small individual projects to larger team-based projects.\n\nMy current focus is mainly on the areas of DevOps &amp; Serverless. But once in a while, I will also be branching off for some interesting projects in ML, AI, Blockchain and IoT. Wherever the tech takes me really!\n\nThrough testing and trying these projects I have gained experience in Java, Python, Swift, Objective C, Cuda, C, C++, Go, and Javascript (React, Node, etc.).\n***\n\n\n  Brian Mathews @DevOps4Days\n\n  Developer Advocate at Oracle."
					}
					
				
			
		
			
				
					,
					
					"integration-what-is-api-gateway-and-why-you-need-on": {
						"id": "integration-what-is-api-gateway-and-why-you-need-on",
						"title": "What is API gateway and why you need one?",
						"categories": "Integration",
						"url": " /integration/what-is-api-gateway-and-why-you-need-on/",
						"content": "We can say APIs are everywhere nowadays. Likely any app you build now consumes data from other apps, your own or from 3rd parties. That means you need to expose your app services to the world as well.\n\n\n\nWhy I need an API gateway?\n\nHow many requests are you expecting per second? How many do you want to allow per minute? What happens if those requests are invalid? What if they are dangerous? Do you need authentication? What about authorization? I’m sure you want TLS on, right?\n\nLet’s assume you know the answer to those questions. Now it’s time to implement all of those validations, rate limiting, auto-scaling, filtering out bad requests, integrate with OAuth, maybe OpenID Connect, TLS termination, and so on.\n\nAren’t all those cross-cutting aspects? Do we want to pollute our business code with that extra noise? The approach to these concerns should be the same across all contributors and teammates implementing the API.\n\nLet’s be honest, it is not rocket science but as soon as we start implementing these new features, someone is going ask for configuration parameters and visibility of the process. At that point, it might be out of hand.\n\nSome of the APIs endpoints would be public, other private. That would change and someone has to implement and enforce those policies. If you are building layers of microservices to provide internal and external services for other apps. It’s getting complicated.\n\nLet’s try an API gateway\n\nWhat if we centralize the APIs of all your apps in a single place where you can configure non-functional requirements and manage policies, handle all the request per second and limit them to a threshold. Monitoring the result. The same way a reverse proxy works the API gateway can redirect the requests to the correct app inside your infrastructure. Imagine that you can handle TLS termination there to simplify the logic in your code. You don’t need to handle all the authentication and authorization in every app independently.\n\nThe good news is that API Gateway is exactly what you need. Manage hundreds of APIs from different systems in a centralized place. Even for a single API I would recommend it, you gain some insights about what the users are doing with your APIs and stop attacks in a lot of scenarios.\n\nEven more, after a few months your API is getting traction, maybe you can implement some extra features for those that are willing to pay for some advanced features. API gateway can give you an easy way to analyze the requests and even monetize those fancy new ideas.\n\nTrade-offs\n\nNow that you are happy bringing in a new API gateway, you have to make sure the routing is correct at deployment time. You don’t want to have a call because some user can reach out your API.\n\nYour API gateway should have the capacity to handle all the requests and be resilient. Think about what infrastructure is needed. Load balancer, metrics and alerts, and so on. Keep in mind that the complexity doesn’t disappear, it is moved to a place where you can manage more easily.\n\nTry in the cloud\n\nSome services in the cloud are available to handle all of this complexity for you. If you want to explore some options, feel free to play around with API platform cloud. It’s part of the catalog of cloud services that Oracle offers. See more on API Platform.\n\nCreate a free trial account!\n\nHappy hacking!\n\n\n\n\n  Victor Martin @victorilloleon\n\n  Software engineer and Oracle developer advocate.\nLong fancy name for just a guy who enjoys writing code and see it running in the cloud."
					}
					
				
			
		
			
				
					,
					
					"kubernetes-k8s-cluster-through-ui": {
						"id": "kubernetes-k8s-cluster-through-ui",
						"title": "Deploying a Kubernetes Cluster through a UI",
						"categories": "Kubernetes",
						"url": " /kubernetes/k8s-cluster-through-ui/",
						"content": "Introduction\n\nKubernetes (aka k8s) is an open-source system for orchestrating containerized applications. It allows automating deployment, scaling, and management of clusters for running cloud-native applications.\nIn this tutorial, I am going to show you how it is possible to deploy a simple cluster on the cloud with a few clicks through the Oracle Cloud Infrastructure (OCI) Web console. For this purpose, I am going to use the Container Engine for Kubernetes (OKE) available under the “Developer Services” section within “Compute” services.\n\n\n\nPrerequisites\n\nBefore creating a new cluster, it is important that all the prerequisites listed here are fulfilled. Assuming we are Administrators in the tenancy, it is necessary to create the following policy under the root compartment:\n\nallow service OKE to manage all-resources in tenancy\n\n\nThis policy will allow OKE to manage networks, instances, storage, load balancers, etc. on our behalf.\n\nFor this tutorial, we also need to have OCI CLI and kubectl on our local machine.\n\nDeploying a new cluster\n\nIn this tutorial I am going to use the OCI datacenter located in Frankfurt, therefore the OKE console is reachable at “https://console.eu-frankfurt-1.oraclecloud.com/containers/clusters” in this case.\n\nThe next step is to click on the “Create Cluster” button and the following interface will show up:\n\n\n\nBy choosing the “Quick create” option, all I need to enter is the Cluster name and Kubernetes Version (v1.11.5 at this time). \n\nOKE by default creates a new Virtual Cloud Network (VCN) with 2 subnets for Load Balancers and 3 subnets for master and worker nodes, an Internet Gateway, a route table, and 2 Security lists. In addition, it creates a new node pool with 1 node for each subnet (3 nodes in total in this case). The number of nodes for each subnet can be increased at this point or can be scaled up later, without service interruptions.\n\nBefore clicking on “Create” button, I ticked the two checkboxes that allow enabling Kubernetes dashboard and Tiller (Helm) on the cluster. The former is useful for debugging and monitoring purposes, while the latter allows having Tiller installed and configured automatically for us.\n\n\n\nUpon clicking on “Create”, OKE will start the creation and deployment of the new cluster on Oracle Cloud.\n\nConnecting to the cluster\n\nUsing the OCI CLI, it is possible to download the Kubeconfig file generated by OKE and use kubectl to manage the cluster.\n\nIn the cluster landing page, after clicking on “Access Kubeconfig”, a popup window will show the commands to run on our local machine in order to download the kubeconfig file. In my case the commands are as follows:\n\n# Create a folder for storing kubeconfig file  \nmkdir -p $HOME/.kube\n\n# Use OCI CLI to download the kubeconfig file on the local machine  \noci ce cluster create-kubeconfig — cluster-id ocid1.cluster.oc1.eu-frankfurt-1.aaaaaaaaaezgkzraasadasdassdzrwhe2gcobzgcztmnjwgjqt — file $HOME/.kube/config — region eu-frankfurt-1\n\n\nIt is also necessary to point the KUBECONFIG environment variable to the _kubeconfig_file downloaded previously.\n\n# Export the environment variable  \nexport KUBECONFIG=$HOME/.kube/config\n\n\nNow I can run the following two commands to get more information about the cluster (these are also displayed in the Web console):\n\n# Get info about k8s nodes  \nkubectl get nodes\n\n# Get info about k8s cluster  \nkubectl cluster-info\n\n\nThey will show information about the 3 nodes IPs and master Load Balancer address. These can be useful in case we want to connect to any of the nodes via SSH.\n\nUsing kubectl it is also possible to run a proxy on the local machine to connect to the Kubernetes dashboard:\n\nkubectl proxy\n\n\nNow I can use any web browser and navigate to http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\n\n\n\nThe dashboard shows many details about the cluster, including pods, services, nodes, persistent volumes, and deployments and also allows to perform operations via a web interface. As you can see from the screenshot above, Tiller is one of the services running on the cluster and has a Cluster IP assigned to.\n\nTry for free on Oracle Cloud\n\nOKE is only one of the great services offered by Oracle Cloud. There are many others that can be explored to enhance your cloud-native applications.\n\nYou can try the steps I described in this tutorial by registering a free trial account here, and take the opportunity to discover many other services offered by OCI.\n\nTry Oracle Cloud for 30 days!\n\n\n\n\n  Luca Iannario @liannario\n\n  Developer Advocate at Oracle. \nWhen I am not on the clouds, I like travelling and taking pictures."
					}
					
				
			
		
			
				
					,
					
					"contributors-about-me-victor-martin": {
						"id": "contributors-about-me-victor-martin",
						"title": "Victor Martin",
						"categories": "Contributors",
						"url": " /contributors/about-me-victor-martin/",
						"content": "About me\n\nHi all. I’m Victor Martin, software engineer and Oracle cloud developer advocate. Long fancy name for just a guy who enjoys writing code and sees it running in the cloud. Take a look in what we doDeveloper Advocacy team within Oracle.\n\n\n\nWhat is a developer advocate for me?\nFor me, a developer advocate is a technologist who enjoys sharing experiences and different approaches to do things. Introduce other developers to new technologies and minimize the learning curve that we suffer the first time we face a new technology.\n\nI do this for myself as well. There is nothing better than expose your knowledge to others to keep learning and challenge yourself with questions that reach your blind spots that you are not even aware of. That is brilliant!\n\nWhat I do\nI’m part of a new team at Oracle with a great mission: share our experience building cloud-native applications and talk about new ways to do things all cloud!\n\nIn the +10 years as a developer, I’ve been writing code in different fields like air traffic management, financial services, integration, security for APIs, etc. Over these years I’ve seen many technical and organizational challenges, probably not very different from those you have.\n\n\n\nIt’s difficult to mention just a few technologies these days where everything is changing so quickly and new exciting technologies come up on daily basis but I do enjoy talking about distributed systems, integration, and messaging. Recently, I bumped into event sourcing because it might make my life easy working with microservices. Want to know more? Reach me out!\n\nSometimes I build react web applications, ship them on Docker images and deploy them with Kubernetes… but only the good days though.\n\nI also enjoy bringing new people to the incredible underwater world, so in my spare time, I’m a Divemaster helping a local dive centre to introduce new people to this amazing sport.\n\nBut we are here to talk about software development. So I want to make sure you have fun developing your projects because if not… what’s this all been about? :)\n\n\n\n\n  Victor Martin @victorilloleon\n\n  Software engineer and Oracle developer advocate.\nLong fancy name for just a guy who enjoys writing code and see it running in the cloud."
					}
					
				
			
		
			
				
					,
					
					"contributors-about-me-raquel": {
						"id": "contributors-about-me-raquel",
						"title": "Raquel Teresa",
						"categories": "Contributors",
						"url": " /contributors/about-me-raquel/",
						"content": "About Me\n\nI am part of the Developer Advocacy team at Oracle Digital. I graduated in Telecommunication Engineering in 2017 at the Polytechnic University of Madrid, after completing my thesis on “Implementation of a portal to optimize the quality of the processes in a user management system”. Then I’ve had different IT roles, gaining experience with the challenges that companies and organizations face every day, and realizing how technology can make their lives better.\n\nNew trends such as Blockchain, Machine Learning or Big data (and the Cloud underneath), are already being disruptive in any organization that doesn’t want to be apart from the digital transformation journey. The adoption of these technologies will change businesses, economies, societies, and our lives, ultimately. That’s why I am passionate about technology and people. We have the power to change the status quo.\n\nOur main goal as Developer Advocates is to get in touch with you, the developer community, and contribute to the understanding of the big possibilities these technologies have, by providing new content, tutorials or sample code.\n\nPlease, feel free to reach out your recommendations, feedback or any other comments. We would like an open and interactive channel.\n\n\n\n\n  Raquel Teresa @Raquel_Teresa\n\n  Cloud Sales Consultant and Developer Advocate @ Oracle Digital.\nPassionate about technology and people."
					}
					
				
			
		
			
				
					,
					
					"contributors-about-me-luca": {
						"id": "contributors-about-me-luca",
						"title": "Luca Iannario",
						"categories": "Contributors",
						"url": " /contributors/about-me-luca/",
						"content": "About me\n\nHello everyone, my name is Luca Iannario and I am part of the Developer Advocacy team within Oracle, based in Dublin (Ireland).\n\nMy background is in Computer Engineering (MSc) and I started my career as a Software Engineer building enterprise distributed and mobile applications while living in Rome (Italy). Since 2015 I live in Dublin where I started working for a Pharma company as Solutions Architect, leading a team of developers and operations sparse across the globe, working mainly on open-source technologies and cloud platforms. This opportunity allowed me to work more closely with developers coming from different communities, backgrounds, and cultures, understanding their needs and, most importantly, collaborating with them to find smart solutions for challenging projects.\n\n\nPhoto by John Schnobrich\n\nThis is what the Developer Advocacy team has been created for: being closer to the developer communities, facilitating the knowledge and adoption of emerging technologies, and establishing a bi-directional and trusted communication channel that allows to improve the products we use to implement great solutions, as well as consulting and helping them to identify the best architecture that suits their needs. A number of initiatives are going to be supported, including tutorials, videos, code samples, meetups, and conferences.\n\nAs a result, this team has the ultimate goal of increasing our customer’s adoption and innovation speed, while making any business more productive and focused on reaching its targets faster and in a more agile way.\n\nMy areas of interest are about distributed systems and networking, cloud infrastructure, IT Security, CDN, Web and mobile applications, full-stack development, and DevOps. I also sponsor and support open-source software and I have been an active member of the Drupal (Content Management System) community since 2015.\n\nI look forward to new opportunities and to connect with new technology enthusiasts. In the meantime, feel free to get in touch on LinkedIn and Twitter.\n\n\n\n\n  Luca Iannario @liannario\n\n  Developer Advocate at Oracle. \nWhen I am not on the clouds, I like travelling and taking pictures."
					}
					
				
			
		
	};
</script>
<script src="/js/lunr.min.js"></script>
<script src="/js/search.js"></script>
			</div>
		</section>

		<footer>
	<div class="wrapper">
		<p class="edit-footer"><a class="editor-link btn" href="cloudcannon:collections/_data/footer.yml" class="btn" style="padding: 5px;"><strong>&#9998;</strong> Edit footer</a></p>
		<ul class="footer-links">
			
				<li><a target="_blank" href="https://facebook.com/OracleDevs" class="Facebook-icon">
					
						
		<svg class="facebook" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M19,4V7H17A1,1 0 0,0 16,8V10H19V13H16V20H13V13H11V10H13V7.5C13,5.56 14.57,4 16.5,4M20,2H4A2,2 0 0,0 2,4V20A2,2 0 0,0 4,22H20A2,2 0 0,0 22,20V4C22,2.89 21.1,2 20,2Z" /></svg>
	

					
					</a></li>
			
				<li><a target="_blank" href="https://twitter.com/oracledevs" class="Twitter-icon">
					
						
		<svg class="twitter" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M22.46,6C21.69,6.35 20.86,6.58 20,6.69C20.88,6.16 21.56,5.32 21.88,4.31C21.05,4.81 20.13,5.16 19.16,5.36C18.37,4.5 17.26,4 16,4C13.65,4 11.73,5.92 11.73,8.29C11.73,8.63 11.77,8.96 11.84,9.27C8.28,9.09 5.11,7.38 3,4.79C2.63,5.42 2.42,6.16 2.42,6.94C2.42,8.43 3.17,9.75 4.33,10.5C3.62,10.5 2.96,10.3 2.38,10C2.38,10 2.38,10 2.38,10.03C2.38,12.11 3.86,13.85 5.82,14.24C5.46,14.34 5.08,14.39 4.69,14.39C4.42,14.39 4.15,14.36 3.89,14.31C4.43,16 6,17.26 7.89,17.29C6.43,18.45 4.58,19.13 2.56,19.13C2.22,19.13 1.88,19.11 1.54,19.07C3.44,20.29 5.7,21 8.12,21C16,21 20.33,14.46 20.33,8.79C20.33,8.6 20.33,8.42 20.32,8.23C21.16,7.63 21.88,6.87 22.46,6Z" /></svg>
	

					
					</a></li>
			
				<li><a target="_blank" href="https://www.youtube.com/channel/UCPpdGsinMIzZkJ7HOrg5ANA/featured" class="YouTube-icon">
					
						
		<svg class="youtube" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M10,16.5V7.5L16,12M20,4.4C19.4,4.2 15.7,4 12,4C8.3,4 4.6,4.19 4,4.38C2.44,4.9 2,8.4 2,12C2,15.59 2.44,19.1 4,19.61C4.6,19.81 8.3,20 12,20C15.7,20 19.4,19.81 20,19.61C21.56,19.1 22,15.59 22,12C22,8.4 21.56,4.91 20,4.4Z" /></svg>
	

					
					</a></li>
			
				<li><a  href="/feed.xml" class="RSS-icon">
					
						
		<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"/><circle cx="6.18" cy="17.82" r="2.18"/><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/></svg>
		

					
					</a></li>
			
				<li><a  href="/profiles/" >
					
					About Us</a></li>
			
		</ul>
		<p class="copyright">&copy; OD Groundbreakers 2019. All rights reserved.</p>
	</div>
</footer>
		<script>
			$(function() {
				$('a[href*=\\#]').not(".no-smooth").on('click', function(event){
					var el = $(this.hash);
					if (el.length > 0) {
						// event.preventDefault();
						$('html,body').animate({scrollTop:$(this.hash).offset().top - 50}, 500);
					}
				});

				$('svg').click(function() {
					$(this).parent('form').submit();
				});
			});

			document.getElementById("open-nav").addEventListener("click", function (event) {
				event.preventDefault();
				document.body.classList.toggle("nav-open");
			});
		</script>
	</body>
</html>
